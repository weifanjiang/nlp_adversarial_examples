{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils\n",
    "import glove_utils\n",
    "import models\n",
    "import display_utils\n",
    "from goog_lm import LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lm_data_utils\n",
    "import lm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1001)\n",
    "tf.set_random_seed(1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE  = 50000\n",
    "with open('aux_files/dataset_%d.pkl' %VOCAB_SIZE, 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_len = [len(dataset.test_seqs2[i]) for i in \n",
    "           range(len(dataset.test_seqs2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mat = np.load('aux_files/dist_counter_%d.npy' %VOCAB_SIZE)\n",
    "# Prevent returning 0 as most similar word because it is not part of the dictionary\n",
    "dist_mat[0,:] = 100000\n",
    "dist_mat[:,0] = 100000\n",
    "\n",
    "skip_list = np.load('aux_files/missed_embeddings_counter_%d.npy' %VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating how we find the most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest to `later` are:\n",
      " --  subsequent   0.18323109771400015\n",
      " --  subsequently   0.1867195991340007\n",
      " --  afterward   0.2509214012219996\n",
      " --  afterwards   0.2576958961479996\n",
      " --  thereafter   0.2741981096589998\n",
      " --  trailing   0.3368002712810001\n",
      " --  after   0.34520261237799876\n",
      " --  then   0.36472839338299834\n",
      " --  posterior   0.4310855888389997\n",
      " --  following   0.4833073676040003\n",
      "----\n",
      "Closest to `takes` are:\n",
      " --  pick   0.31130546563200046\n",
      " --  taking   0.42471158462800007\n",
      " --  picked   0.48527412495900113\n",
      "----\n",
      "Closest to `instead` are:\n",
      " --  conversely   0.30340380498499964\n",
      " --  however   0.3475382865829997\n",
      " --  alternatively   0.39540487543000014\n",
      " --  alternately   0.4439627395600003\n",
      " --  nevertheless   0.477163975792001\n",
      "----\n",
      "Closest to `seem` are:\n",
      " --  seems   0.007052995653001215\n",
      " --  appears   0.32837244735200044\n",
      " --  looks   0.33534638306400066\n",
      " --  transpires   0.456207185493001\n",
      "----\n",
      "Closest to `beautiful` are:\n",
      " --  gorgeous   0.019236443661999614\n",
      " --  wonderful   0.10149643378299977\n",
      " --  splendid   0.10299021060599989\n",
      " --  handsome   0.11803810151499938\n",
      " --  resplendent   0.11808701124700005\n",
      " --  wondrous   0.12150066282500016\n",
      " --  marvelous   0.12519975539099892\n",
      " --  marvellous   0.1283983423189996\n",
      " --  fantastic   0.1362662712549989\n",
      " --  sumptuous   0.14140581277100006\n",
      " --  magnificent   0.1463668716009996\n",
      " --  terrific   0.15607668417800036\n",
      " --  lovely   0.1600076271229991\n",
      " --  ravishing   0.17404625231200033\n",
      " --  sublime   0.17909557696099943\n",
      " --  exquisite   0.1881071417279998\n",
      " --  fabulous   0.20363493095600038\n",
      " --  delightful   0.20468405530899902\n",
      " --  superb   0.21660537682999959\n",
      " --  excellent   0.22435712285300036\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for i in range(300, 305):\n",
    "    src_word = i\n",
    "    nearest, nearest_dist = glove_utils.pick_most_similar_words(src_word, dist_mat,20, 0.5)\n",
    "        \n",
    "    print('Closest to `%s` are:' %(dataset.inv_dict[src_word]))\n",
    "    for w_id, w_dist in zip(nearest, nearest_dist):\n",
    "          print(' -- ', dataset.inv_dict[w_id], ' ', w_dist)\n",
    "\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 250\n",
    "train_x = pad_sequences(dataset.train_seqs2, maxlen=max_len, padding='post')\n",
    "train_y = np.array(dataset.train_y)\n",
    "test_x = pad_sequences(dataset.test_seqs2, maxlen=max_len, padding='post')\n",
    "test_y = np.array(dataset.test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/weifanjiang/Documents/CU/COMS6998-10/project/nlp_adversarial_examples/models.py:19: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/weifanjiang/Documents/CU/COMS6998-10/project/nlp_adversarial_examples/models.py:24: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/weifanjiang/Documents/CU/COMS6998-10/project/nlp_adversarial_examples/models.py:28: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /Users/weifanjiang/Documents/CU/COMS6998-10/project/nlp_adversarial_examples/models.py:36: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/weifanjiang/Documents/CU/COMS6998-10/project/nlp_adversarial_examples/models.py:54: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./models/imdb_model\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "if tf.get_default_session():\n",
    "    sess.close()\n",
    "sess = tf.Session()\n",
    "batch_size = 1\n",
    "lstm_size = 128\n",
    "#max_len =  100\n",
    "\n",
    "with tf.variable_scope('imdb', reuse=False):\n",
    "    model = models.SentimentModel(batch_size=batch_size,\n",
    "                           lstm_size = lstm_size,\n",
    "                           max_len = max_len,\n",
    "                           embeddings_dim=300, vocab_size=dist_mat.shape[1],is_train = False)\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, './models/imdb_model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Google Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM start vocab loading \n",
      "WARNING:tensorflow:From /Users/weifanjiang/Documents/CU/COMS6998-10/project/nlp_adversarial_examples/lm_data_utils.py:40: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "LM vocab loading done\n",
      "WARNING:tensorflow:From /Users/weifanjiang/Documents/CU/COMS6998-10/project/nlp_adversarial_examples/lm_utils.py:21: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n",
      "WARNING:tensorflow:From /Users/weifanjiang/Documents/CU/COMS6998-10/project/nlp_adversarial_examples/lm_utils.py:23: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recovering graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/weifanjiang/Documents/CU/COMS6998-10/project/nlp_adversarial_examples/lm_utils.py:26: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "INFO:tensorflow:Recovering Graph goog_lm/graph-2016-09-10.pbtxt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recovering checkpoint goog_lm/ckpt-*\n"
     ]
    }
   ],
   "source": [
    "goog_lm = LM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### demonstrating the GoogLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest to `play` are ['playing', 'gaming', 'games', 'toy', 'playback', 'game', 'plaything', 'cheek', 'gambling', 'toys', 'toying', 'replay', 'stake', 'plays', 'jeu', 'gamble', 'staking', 'reproduction', 'casino', 'sets']\n"
     ]
    }
   ],
   "source": [
    "src_word = dataset.dict['play']\n",
    "nearest, nearest_dist = glove_utils.pick_most_similar_words(src_word, dist_mat,20)\n",
    "nearest_w = [dataset.inv_dict[x] for x in nearest]\n",
    "print('Closest to `%s` are %s' %(dataset.inv_dict[src_word], nearest_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most probable is  game\n"
     ]
    }
   ],
   "source": [
    "prefix = 'is'\n",
    "suffix = 'with'\n",
    "lm_preds = goog_lm.get_words_probs(prefix, nearest_w, suffix)\n",
    "print('most probable is ', nearest_w[np.argmax(lm_preds)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attacks import GeneticAtack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Attack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = 60\n",
    "n1 = 8\n",
    "\n",
    "with tf.variable_scope('imdb', reuse=True):\n",
    "    batch_model = models.SentimentModel(batch_size=pop_size,\n",
    "                           lstm_size = lstm_size,\n",
    "                           max_len = max_len,\n",
    "                           embeddings_dim=300, vocab_size=dist_mat.shape[1],is_train = False)\n",
    "    \n",
    "with tf.variable_scope('imdb', reuse=True):\n",
    "    neighbour_model = models.SentimentModel(batch_size=n1,\n",
    "                           lstm_size = lstm_size,\n",
    "                           max_len = max_len,\n",
    "                           embeddings_dim=300, vocab_size=dist_mat.shape[1],is_train = False)\n",
    "ga_atttack = GeneticAtack(sess, model, batch_model, neighbour_model, dataset, dist_mat, \n",
    "                                  skip_list,\n",
    "                                  goog_lm, max_iters=30, \n",
    "                                   pop_size=pop_size,\n",
    "                                  \n",
    "                                  n1 = n1,\n",
    "                                  n2 = 4,\n",
    "                                 use_lm = True, use_suffix=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 5000\n",
    "TEST_SIZE = 20\n",
    "test_idx = np.random.choice(len(dataset.test_y), SAMPLE_SIZE, replace=False)\n",
    "test_len = []\n",
    "for i in range(SAMPLE_SIZE):\n",
    "    test_len.append(len(dataset.test_seqs2[test_idx[i]]))\n",
    "print('Shortest sentence in our test set is %d words' %np.min(test_len))\n",
    "\n",
    "test_list = []\n",
    "orig_list = []\n",
    "orig_label_list = []\n",
    "adv_list = []\n",
    "dist_list = []\n",
    "\n",
    "for i in range(TEST_SIZE):\n",
    "    x_orig = test_x[test_idx[i]]\n",
    "    orig_label = test_y[test_idx[i]]\n",
    "    orig_preds=  model.predict(sess, x_orig[np.newaxis, :])[0]\n",
    "    # print(orig_label, orig_preds, np.argmax(orig_preds))\n",
    "    if np.argmax(orig_preds) != orig_label:\n",
    "        #print('skipping wrong classifed ..')\n",
    "        #print('--------------------------')\n",
    "        continue\n",
    "    x_len = np.sum(np.sign(x_orig))\n",
    "    if x_len >= 100:\n",
    "        #print('skipping too long input..')\n",
    "        #print('--------------------------')\n",
    "        continue\n",
    "    # if np.max(orig_preds) < 0.90:\n",
    "    #    print('skipping low confidence .. \\n-----\\n')\n",
    "    #    continue\n",
    "    print('****** ', len(test_list) + 1, ' ********')\n",
    "    test_list.append(test_idx[i])\n",
    "    orig_list.append(x_orig)\n",
    "    target_label = 1 if orig_label == 0 else 0\n",
    "    orig_label_list.append(orig_label)\n",
    "    print(\"start attack\")\n",
    "    x_adv = ga_atttack.attack( x_orig, target_label)\n",
    "    print(\"finissh attack\")\n",
    "    adv_list.append(x_adv)\n",
    "    if x_adv is None:\n",
    "        print('%d failed' %(i+1))\n",
    "        dist_list.append(100000)\n",
    "    else:\n",
    "        num_changes = np.sum(x_orig != x_adv)\n",
    "        print('%d - %d changed.' %(i+1, num_changes))\n",
    "        dist_list.append(num_changes)\n",
    "        # display_utils.visualize_attack(sess, model, dataset, x_orig, x_adv)\n",
    "    print('--------------------------')\n",
    "    if (len(test_list)>= TEST_SIZE):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Compute Attack success rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_len = [np.sum(np.sign(x)) for x in orig_list]\n",
    "normalized_dist_list = [dist_list[i]/orig_len[i] for i in range(len(orig_list)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack success rate : 92.00%\n",
      "Median percentange of modifications: 6.45% \n",
      "Mean percentange of modifications: 9.14% \n"
     ]
    }
   ],
   "source": [
    "SUCCESS_THRESHOLD  = 0.25\n",
    "successful_attacks = [x < SUCCESS_THRESHOLD for x in normalized_dist_list]\n",
    "print('Attack success rate : {:.2f}%'.format(np.mean(successful_attacks)*100))\n",
    "print('Median percentange of modifications: {:.02f}% '.format(\n",
    "    np.median([x for x in normalized_dist_list if x < 1])*100))\n",
    "print('Mean percentange of modifications: {:.02f}% '.format(\n",
    "    np.mean([x for x in normalized_dist_list if x < 1])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Prediction = Negative. (Confidence = 99.81) \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "poorly directed short film <b style='color:green'>shot</b> on hi def or betacam it appears it <b style='color:green'>screams</b> student film video all the way the <b style='color:green'>premise</b> is limited in scope and the short actually feels a lot <b style='color:green'>longer</b> than it runs some interesting acting moments and some decent production value but not enough to lift this film from the hole it has fallen <b style='color:green'>into</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------  After attack -------------\n",
      "New Prediction = Positive. (Confidence = 59.80) \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "poorly directed short film <b style='color:red'>gunshot</b> on hi def or betacam it appears it <b style='color:red'>shrieks</b> student film video all the way the <b style='color:red'>premises</b> is limited in scope and the short actually feels a lot <b style='color:red'>longest</b> than it runs some interesting acting moments and some decent production value but not enough to lift this film from the hole it has fallen <b style='color:red'>in</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visual_idx = np.random.choice(len(orig_list))\n",
    "display_utils.visualize_attack(sess, model, dataset, orig_list[visual_idx], adv_list[visual_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Prediction = Positive. (Confidence = 85.73) \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "absolutely fantastic whatever i say wouldn't do this <b style='color:green'>underrated</b> movie the justice it <b style='color:green'>deserves</b> watch it now fantastic"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------  After attack -------------\n",
      "New Prediction = Negative. (Confidence = 50.19) \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "absolutely fantastic whatever i say wouldn't do this <b style='color:red'>underestimated</b> movie the justice it <b style='color:red'>deserve</b> watch it now fantastic"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visual_idx = np.random.choice(len(orig_list))\n",
    "display_utils.visualize_attack(sess, model, dataset, orig_list[visual_idx], adv_list[visual_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save success\n",
    "with open('attack_results_final.pkl', 'wb') as f:\n",
    "    pickle.dump((test_list, orig_list, orig_label_list, adv_list, normalized_dist_list), f)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
